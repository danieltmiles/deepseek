{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7714c7-033f-4348-9793-e4b010f0a387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache\n",
    "import random\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5b254a-5419-4f5c-a79f-2c14f2d1923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=\"auto\",\n",
    "    # attn_implementation=\"eager\",\n",
    ")\n",
    "model = model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06ac884-48f8-40d1-81e1-94f5484088af",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _start_think_token, end_think_token = tokenizer.encode(\"<think></think>\")\n",
    "prefill = \"\"\n",
    "replacements = [\"\\nWait, but\", \"\\nHmm\", \"\\nSo\"]\n",
    "\n",
    "@torch.inference_mode\n",
    "def reasoning_effort(question: str, min_thinking_tokens: int, max_thinking_tokens: int = 500):\n",
    "    tokens = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": \"<think>\\n\" + prefill},\n",
    "        ],\n",
    "        continue_final_message=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    decoded_prompt = tokenizer.decode(tokens[0])\n",
    "    tokens = tokens.to(model.device)\n",
    "    kv = DynamicCache()\n",
    "    n_thinking_tokens = 0\n",
    "\n",
    "    yield tokenizer.decode(list(tokens[0]))\n",
    "    finished_thinking = False\n",
    "    while True:\n",
    "        out = model(input_ids=tokens, past_key_values=kv, use_cache=True)\n",
    "        next_token = torch.multinomial(\n",
    "            torch.softmax(out.logits[0, -1, :], dim=-1), 1\n",
    "        ).item()\n",
    "        kv = out.past_key_values\n",
    "\n",
    "        if (\n",
    "            next_token in (end_think_token, model.config.eos_token_id)\n",
    "            and n_thinking_tokens < min_thinking_tokens\n",
    "        ):\n",
    "            replacement = random.choice(replacements)\n",
    "            print(f\"\\n======================================================\\nmodel tried to stop thinking with {n_thinking_tokens} tokens, less that the specified minimum thinking effort, {min_thinking_tokens}. Replacing </think> token with {replacement}\\n======================================================\\n\")\n",
    "            yield replacement\n",
    "            replacement_tokens = tokenizer.encode(replacement)\n",
    "            n_thinking_tokens += len(replacement_tokens)\n",
    "            tokens = torch.tensor([replacement_tokens]).to(tokens.device)\n",
    "        elif next_token == model.config.eos_token_id:\n",
    "            print(f\"\\n======================================================\\nmodel reached eos token after {n_thinking_tokens} tokens\\n======================================================\\n\")\n",
    "            break\n",
    "        elif not finished_thinking and n_thinking_tokens > max_thinking_tokens:\n",
    "            finished_thinking = True\n",
    "            print(f\"\\n======================================================\\nforcing </think> token after {n_thinking_tokens}\\n======================================================\\n\")\n",
    "            yield \"</think>\"\n",
    "            replacement_tokens = tokenizer.encode(\"</think>\")\n",
    "            n_thinking_tokens += len(replacement_tokens)\n",
    "            tokens = torch.tensor([replacement_tokens]).to(tokens.device)\n",
    "        else:\n",
    "            if next_token == end_think_token:\n",
    "                finished_thinking = True\n",
    "                print(f\"\\n======================================================\\nfinished thinking after {n_thinking_tokens} tokens\\n======================================================\\n\")\n",
    "            yield tokenizer.decode([next_token])\n",
    "            n_thinking_tokens += 1\n",
    "            tokens = torch.tensor([[next_token]]).to(tokens.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43172918-82de-42df-a7c6-0faae7115806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "question = \"Using English, only, please write a short story, around 500 words, or so.\"\n",
    "min_thinking_tokens = 100\n",
    "max_thinking_tokens = 500\n",
    "start = time.time()\n",
    "for chunk in reasoning_effort(question, min_thinking_tokens, max_thinking_tokens):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "end = time.time()\n",
    "print(f\"produced answer in {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21559f2-3589-4211-8d0f-7f56095b322c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381187e3-59cb-451c-9e4b-c9cab31c08cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
